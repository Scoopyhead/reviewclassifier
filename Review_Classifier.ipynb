{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchmetrics\n",
    "\n",
    "import torch.nn.functional as F\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from torchvision import transforms\n",
    "from torch.nn.utils.rnn import pad_sequence, pack_padded_sequence, pad_packed_sequence\n",
    "\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "import ast\n",
    "import torchtext\n",
    "from torchtext.data import get_tokenizer\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "from torchmetrics import F1Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = pd.read_csv('x_rate_train.csv')\n",
    "y = pd.read_csv('y_rate_train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test = pd.read_csv('x_rate_test.csv')\n",
    "y_test = pd.read_csv('y_rate_test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def convert_str_to_list(df, column_name):\n",
    "    df[column_name] = df[column_name].apply(ast.literal_eval)\n",
    "    return df\n",
    "\n",
    "\n",
    "# 转换列\n",
    "x['lemmatized_text_with_pos'] = x['lemmatized_text_with_pos'].astype(str)\n",
    "df = convert_str_to_list(x, 'lemmatized_text_with_pos')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_str_to_list(df, column_name):\n",
    "    def eval_literal(row):\n",
    "        try:\n",
    "            return ast.literal_eval(row)\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to convert: {row}\")\n",
    "            raise e\n",
    "\n",
    "    df[column_name] = df[column_name].apply(eval_literal)\n",
    "    return df\n",
    "\n",
    "# Apply this to your DataFrame\n",
    "x_test['lemmatized_text_with_pos'] = x_test['lemmatized_text_with_pos'].astype(str)\n",
    "df_test = convert_str_to_list(x_test, 'lemmatized_text_with_pos')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df =pd.concat((df,y),axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = pd.concat((df_test, y_test), axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def build_vocab(train_df, min_freq):\n",
    "    unk_token = '<UNK>'\n",
    "#     pad_token = '<PAD>'\n",
    "#     default_index = -1\n",
    "    tokenizer = get_tokenizer(\"basic_english\")\n",
    "    \n",
    "    vocab = build_vocab_from_iterator(train_df[\"lemmatized_text_with_pos\"], min_freq= min_freq, specials=[unk_token])\n",
    "    vocab.set_default_index(vocab[unk_token])\n",
    "    return vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = build_vocab(train_df, min_freq=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5310"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vocab.get_itos())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def token2num(df, vocab):\n",
    "    new_df = pd.DataFrame(columns=('lemmatized_text_with_pos', 'usefulCount', 'reviewCount', 'flagged', 'rate_diff'))\n",
    "    for index, row in df.iterrows():\n",
    "        new_df.loc[index, 'lemmatized_text_with_pos'] = [vocab[w] for w in row['lemmatized_text_with_pos']]\n",
    "        new_df.loc[index, 'flagged'] = row['flagged']\n",
    "        new_df.loc[index, 'usefulCount'] = row['usefulCount']\n",
    "        new_df.loc[index, 'reviewCount'] = row['reviewCount']\n",
    "        new_df.loc[index, 'rate_diff'] = row['rate_diff']\n",
    "    return new_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = token2num(train_df, vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = token2num(test_df, vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_len_train = pd.DataFrame(train_df['lemmatized_text_with_pos'].apply(lambda x: len(x)).copy())\n",
    "max_len_train = df_len_train['lemmatized_text_with_pos'].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_len_test = pd.DataFrame(test_df['lemmatized_text_with_pos'].apply(lambda x: len(x)).copy())\n",
    "max_len_test = df_len_test['lemmatized_text_with_pos'].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyDataset(Dataset):\n",
    "    def __init__(self, df, transform=None):\n",
    "        self.df = df\n",
    "        self.transform = transform\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        x = torch.tensor(self.df.iloc[idx, 0])\n",
    "        y = torch.tensor(self.df.iloc[idx, 3])\n",
    "        uc = torch.tensor(self.df.iloc[idx, 1])\n",
    "        rd = torch.tensor(self.df.iloc[idx, 2])\n",
    "        # rc = torch.tensor(self.df.iloc[idx, 2])\n",
    "        # print(x)\n",
    "        # print(y)\n",
    "        # print(uc)\n",
    "        # print(rc)\n",
    "        if self.transform is not None:\n",
    "            x = self.transform(x)\n",
    "        return x, y, uc, rd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = MyDataset(train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_set = MyDataset(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    data = [item[0] for item in batch]\n",
    "    lens = [len(item[0]) for item in batch] \n",
    "    ucounts = torch.tensor([item[2] for item in batch])\n",
    "    rdiff = torch.tensor([item[3] for item in batch])\n",
    "    targets = torch.tensor([item[1] for item in batch])\n",
    "   \n",
    "    padded_batch = pad_sequence(data, batch_first=True)\n",
    "    return padded_batch, targets, lens, ucounts, rdiff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "train_loader = DataLoader(train_set, shuffle=True, batch_size=batch_size, collate_fn=collate_fn)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loader = DataLoader(test_set, shuffle=False, batch_size=batch_size, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReviewEmbedder(nn.Module):\n",
    "    def __init__(self, vocab_size, text_embed_dim, num_meta_features, meta_dim):\n",
    "        super(ReviewEmbedder, self).__init__()\n",
    "        self.text_embedding = nn.Embedding(num_embeddings=vocab_size, embedding_dim=text_embed_dim)\n",
    "        \n",
    "        # Meta embedding for numerical features like useful_counts and restaurant_ratings\n",
    "        self.meta_embedding = nn.Linear(num_meta_features, meta_dim)\n",
    "\n",
    "    def forward(self, text_indices, numerical_features):\n",
    "        # Embed textual content\n",
    "        text_embedded = self.text_embedding(text_indices)\n",
    "\n",
    "        # Embed numerical features\n",
    "        # Assuming numerical_features is a batch of [useful_counts, restaurant_ratings]\n",
    "        meta_embedded = self.meta_embedding(numerical_features)\n",
    "\n",
    "        # Concatenate embeddings along the last dimension\n",
    "        combined_embeddings = torch.cat((text_embedded, meta_embedded.unsqueeze(1).expand(-1, text_embedded.size(1), -1)), dim=2)\n",
    "        return combined_embeddings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class RNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, n_layers, n_classes, dict_size, embedding_size, num_meta_features):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.embedding_size = embedding_size\n",
    "        self.meta_size = int(self.embedding_size*0.5)\n",
    "        self.dict_size = dict_size\n",
    "        self.embedding = ReviewEmbedder(self.dict_size, self.embedding_size, num_meta_features, self.embedding_size)\n",
    "        \n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.n_layers = n_layers\n",
    "        self.n_classes = n_classes\n",
    "        \n",
    "        # Additional embedding layer for numerical features like usefulCounts\n",
    "        self.meta_embedding = nn.Linear(num_meta_features, self.embedding_size)  # Embedding size for metadata to match text embedding size\n",
    "        \n",
    "        # Attention mechanism\n",
    "        self.attention = nn.MultiheadAttention(embedding_size*2, num_heads=1, dropout=0.1, batch_first = True)\n",
    "        \n",
    "        self.lstm = nn.LSTM(input_size=self.input_size*2 + num_meta_features,  # Adjust input size to include meta embedding\n",
    "                            hidden_size=self.hidden_size, \n",
    "                            num_layers=self.n_layers, \n",
    "                            batch_first=True)\n",
    "        \n",
    "        self.fc = nn.Linear(self.hidden_size, self.n_classes)  # Output layer\n",
    "        \n",
    "        # Normalization layers for numerical features\n",
    "        self.usefulCount_norm = nn.BatchNorm1d(1)  # Assuming usefulCount is a single feature\n",
    "        self.reviewCount_norm = nn.BatchNorm1d(1)  # Assuming res_diff is a single feature\n",
    "\n",
    "    def forward(self, x, lens, numerical_features, max_length):\n",
    "        # Normalization for numerical features\n",
    "        numerical_features = numerical_features.float()\n",
    "        usefulCount_normalized = self.usefulCount_norm(numerical_features[:, 0].unsqueeze(1))\n",
    "        reviewCount_normalized = self.reviewCount_norm(numerical_features[:, 1].unsqueeze(1))\n",
    "\n",
    "        # Combine normalized numerical features\n",
    "        normalized_features = torch.cat((usefulCount_normalized, reviewCount_normalized), dim=1)\n",
    "        embeddings = self.embedding(x, normalized_features)\n",
    "\n",
    "        # Concatenate text embeddings with numerical metadata embeddings\n",
    "        # combined_embeddings_with_metadata = torch.cat([embeddings, normalized_features.unsqueeze(1)], dim=-1)\n",
    "\n",
    "        # Apply self-attention mechanism\n",
    "        attention_output, _ = self.attention(embeddings, embeddings, embeddings)\n",
    "        # Assuming attention_outputs are in the shape (batch, seq_len, features)\n",
    "        padded = torch.nn.functional.pad(attention_output, (0, 0, 0, max_length - attention_output.size(1)), mode='constant', value=0) if attention_output.size(1) < max_length else attention_output[:, :max_length]\n",
    "        attention_outputs_reshaped = padded.reshape(padded.size(0), -1)\n",
    "       \n",
    "        # Concatenate attention output with normalized numerical features\n",
    "        combined_embeddings = torch.cat([attention_output, normalized_features.unsqueeze(1).expand(-1, attention_output.size(1), -1)], dim=-1)\n",
    "        \n",
    "        # LSTM layer\n",
    "        packed_combined_embeddings = pack_padded_sequence(combined_embeddings, lens, batch_first=True, enforce_sorted=False)\n",
    "        packed_output, (ht, ct) = self.lstm(packed_combined_embeddings)\n",
    "\n",
    "        # Output layer\n",
    "        padded_output, lengths = pad_packed_sequence(packed_output, batch_first=True)\n",
    "        h_n = []\n",
    "        for seq, length in zip(padded_output, lengths):\n",
    "            h_n.append(seq[length-1, :])\n",
    "        h_n_batch = torch.stack(h_n)\n",
    "        output = self.fc(h_n_batch)\n",
    "\n",
    "        return output, ht[-1], attention_outputs_reshaped\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epoch, model, train_loader, batch_size, device, loss_fn, optimizer):\n",
    "    model.train()\n",
    "    train_loss = 0.\n",
    "    n_corrects = 0\n",
    "    total = 0\n",
    "    step = 0\n",
    "    for idx, (texts, labels, lens, uc, rd) in enumerate(train_loader):\n",
    "        optimizer.zero_grad()\n",
    "        texts = texts.to(device)\n",
    "        labels = labels.to(device)\n",
    "        uc = uc.to(device)\n",
    "        rd = rd.to(device)\n",
    "        features = torch.cat((uc.unsqueeze(1), rd.unsqueeze(1)), dim=1)\n",
    "        # print(features.shape)\n",
    "        \n",
    "\n",
    "        outputs, h, att = model(texts, lens, features, max_len_train)\n",
    "        loss = loss_fn(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # att = att.detach().cpu().numpy()\n",
    "        train_loss += loss.item()\n",
    "        _, predictions = torch.max(outputs, dim=1)\n",
    "        n_corrects += labels.eq(predictions).sum().item()\n",
    "        total += labels.size(0)\n",
    "        step += 1\n",
    "    train_accuracy = 100. * n_corrects / total\n",
    "#     train_loss /= step\n",
    "    return train_loss, train_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "@torch.no_grad()\n",
    "def test(model, test_loader, device):\n",
    "    n_corrects = 0\n",
    "    total = 0\n",
    "#     embedding = embedding.to(device)\n",
    "    model.eval()\n",
    "\n",
    "    # Metrics\n",
    "    precision = torchmetrics.Precision(task='multiclass', num_classes=2, average='macro').to(device)  # Adjust num_classes as needed\n",
    "    recall = torchmetrics.Recall(task='multiclass', num_classes=2, average='macro').to(device)\n",
    "    f1 = torchmetrics.F1Score(task='multiclass', num_classes=2, average='macro').to(device)\n",
    "    accuracy = torchmetrics.Accuracy(task='multiclass', num_classes=2, average='macro').to(device)\n",
    "    hidden_state = []\n",
    "    all_predictions = []\n",
    "    all_labels = []\n",
    "    att_layer = []\n",
    "\n",
    "    for texts, labels, lens, uc, rd in test_loader:\n",
    "        texts = texts.to(device)\n",
    "        labels = labels.to(device)\n",
    "        uc = torch.tensor(uc, dtype=torch.int).to(device)\n",
    "        rd = torch.tensor(rd, dtype=torch.float).to(device)\n",
    "        features = torch.cat((uc.unsqueeze(1), rd.unsqueeze(1)), dim=1)\n",
    "        # print(features.shape)\n",
    "        \n",
    "        outputs, hidden, attention = model(texts, lens, features, max_len_test)\n",
    "        _, predictions = torch.max(outputs, dim=1)\n",
    "\n",
    "        # Update metrics\n",
    "        precision.update(predictions, labels)\n",
    "        recall.update(predictions, labels)\n",
    "        f1.update(predictions, labels)\n",
    "        accuracy.update(predictions, labels)\n",
    "\n",
    "        n_corrects += labels.eq(predictions).sum().item()\n",
    "        total += labels.size(0)\n",
    "\n",
    "        hidden_state.append(hidden.detach().cpu().numpy())\n",
    "        att_layer.append(attention.detach().cpu().numpy())\n",
    "        # Collect all labels and predictions\n",
    "        all_labels.extend(labels.cpu().numpy())\n",
    "        all_predictions.extend(predictions.cpu().numpy())\n",
    "\n",
    "    # Compute final metric values\n",
    "    final_precision = precision.compute()\n",
    "    final_recall = recall.compute()\n",
    "    final_f1 = f1.compute()\n",
    "    final_accuracy = accuracy.compute() * 100  # Convert to percentage\n",
    "\n",
    "    hidden_states = np.concatenate(hidden_state, axis=0)\n",
    "    att_layers = np.concatenate(att_layer, axis=0)\n",
    "\n",
    "    test_accuracy = 100. * n_corrects / total\n",
    "    # Compute the confusion matrix\n",
    "    cm = confusion_matrix(all_labels, all_predictions)\n",
    "    \n",
    "    # return cm\n",
    "    return test_accuracy, final_accuracy, final_precision, final_recall, final_f1, hidden_states, cm, att_layers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RNN(\n",
      "  (embedding): ReviewEmbedder(\n",
      "    (text_embedding): Embedding(5310, 200)\n",
      "    (meta_embedding): Linear(in_features=2, out_features=200, bias=True)\n",
      "  )\n",
      "  (meta_embedding): Linear(in_features=2, out_features=200, bias=True)\n",
      "  (attention): MultiheadAttention(\n",
      "    (out_proj): NonDynamicallyQuantizableLinear(in_features=400, out_features=400, bias=True)\n",
      "  )\n",
      "  (lstm): LSTM(402, 1024, num_layers=3, batch_first=True)\n",
      "  (fc): Linear(in_features=1024, out_features=2, bias=True)\n",
      "  (usefulCount_norm): BatchNorm1d(1, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (reviewCount_norm): BatchNorm1d(1, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "n_epochs = 30\n",
    "lr = 3e-4\n",
    "\n",
    "embedding_size = 200\n",
    "hidden_size = 1024\n",
    "n_layers = 3\n",
    "dict_size = len(vocab.get_itos())\n",
    "n_classes = 2\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "# print(device)\n",
    "# device = torch.device(\"cpu\")\n",
    "model = RNN(embedding_size, hidden_size, n_layers, n_classes, dict_size, embedding_size, 2).to(device)\n",
    "# model.init_hidden(device)\n",
    "print(model)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/30 | Train Loss: 34.9433 | Acc: 79.81% | lr: 0.000100\n",
      "Epoch: 2/30 | Train Loss: 30.2037 | Acc: 82.71% | lr: 0.000100\n",
      "Epoch: 3/30 | Train Loss: 32.0080 | Acc: 81.78% | lr: 0.000100\n",
      "Epoch: 4/30 | Train Loss: 30.2483 | Acc: 83.24% | lr: 0.000100\n",
      "Epoch: 5/30 | Train Loss: 29.7696 | Acc: 83.39% | lr: 0.000100\n",
      "Epoch: 6/30 | Train Loss: 29.2878 | Acc: 83.33% | lr: 0.000100\n",
      "Epoch: 7/30 | Train Loss: 27.8676 | Acc: 84.47% | lr: 0.000100\n",
      "Epoch: 8/30 | Train Loss: 28.3052 | Acc: 84.56% | lr: 0.000100\n",
      "Epoch: 9/30 | Train Loss: 27.9193 | Acc: 84.32% | lr: 0.000100\n",
      "Epoch: 10/30 | Train Loss: 27.4374 | Acc: 84.39% | lr: 0.000100\n",
      "Epoch: 11/30 | Train Loss: 27.3834 | Acc: 84.44% | lr: 0.000100\n",
      "Epoch: 12/30 | Train Loss: 27.2290 | Acc: 84.65% | lr: 0.000100\n",
      "Epoch: 13/30 | Train Loss: 26.6667 | Acc: 84.99% | lr: 0.000100\n",
      "Epoch: 14/30 | Train Loss: 26.6557 | Acc: 84.97% | lr: 0.000100\n",
      "Epoch: 15/30 | Train Loss: 25.8286 | Acc: 85.32% | lr: 0.000100\n",
      "Epoch: 16/30 | Train Loss: 25.2915 | Acc: 86.04% | lr: 0.000100\n",
      "Epoch: 17/30 | Train Loss: 25.2263 | Acc: 86.38% | lr: 0.000100\n",
      "Epoch: 18/30 | Train Loss: 24.1435 | Acc: 86.79% | lr: 0.000100\n",
      "Epoch: 19/30 | Train Loss: 24.0038 | Acc: 86.80% | lr: 0.000100\n",
      "Epoch: 20/30 | Train Loss: 23.7269 | Acc: 87.32% | lr: 0.000100\n",
      "Epoch: 21/30 | Train Loss: 22.2998 | Acc: 88.25% | lr: 0.000100\n",
      "Epoch: 22/30 | Train Loss: 22.2229 | Acc: 88.26% | lr: 0.000100\n",
      "Epoch: 23/30 | Train Loss: 21.3467 | Acc: 88.79% | lr: 0.000100\n",
      "Epoch: 24/30 | Train Loss: 19.6487 | Acc: 89.87% | lr: 0.000100\n",
      "Epoch: 25/30 | Train Loss: 20.2775 | Acc: 90.04% | lr: 0.000100\n",
      "Epoch: 26/30 | Train Loss: 18.2918 | Acc: 90.78% | lr: 0.000100\n",
      "Epoch: 27/30 | Train Loss: 17.3054 | Acc: 91.42% | lr: 0.000100\n",
      "Epoch: 28/30 | Train Loss: 16.4974 | Acc: 92.03% | lr: 0.000100\n",
      "Epoch: 29/30 | Train Loss: 13.8360 | Acc: 93.52% | lr: 0.000100\n",
      "Epoch: 30/30 | Train Loss: 13.7302 | Acc: 93.96% | lr: 0.000100\n"
     ]
    }
   ],
   "source": [
    "scheduler = optim.lr_scheduler.CyclicLR(optimizer, \n",
    "                                        cycle_momentum=False,\n",
    "                                        base_lr=0.0001,\n",
    "                                        max_lr=0.01,\n",
    "                                        step_size_up=5)\n",
    "for epoch in range(n_epochs):\n",
    "    train_loss, train_accuracy = train(epoch, model, train_loader, batch_size, device, loss_fn, optimizer)\n",
    "    current_lr = optimizer.param_groups[0][\"lr\"]\n",
    "    print(f'Epoch: {epoch+1}/{n_epochs} | Train Loss: {train_loss:.4f} | Acc: {train_accuracy:.2f}% | lr: {current_lr:.6f}')\n",
    "#     scheduler.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1011485/675232125.py:21: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  uc = torch.tensor(uc, dtype=torch.int).to(device)\n",
      "/tmp/ipykernel_1011485/675232125.py:22: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  rd = torch.tensor(rd, dtype=torch.float).to(device)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "test_accuracy, test_acc, test_precision, test_recall, test_f1, hidden_states, cm, attention = test(model, test_loader, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "80.7738814993954"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "80.8004\n",
      "0.8145\n",
      "0.8080\n",
      "0.8068\n"
     ]
    }
   ],
   "source": [
    "print(f\"{test_acc:.4f}\")\n",
    "print(f\"{test_precision:.4f}\")\n",
    "print(f\"{test_recall:.4f}\")\n",
    "print(f\"{test_f1:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlpmixer_pytorch_env37",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
